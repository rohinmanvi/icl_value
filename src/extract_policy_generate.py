#!/usr/bin/env python3
"""
Generate trajectories using a Q-weighted policy extracted from reference + critic.

The extracted policy is:
    π_new(a|s) ∝ π_ref(a|s) · exp(Q(s,a) / τ)

This script:
1. Loads prompts from the dataset
2. Generates new trajectories using the Q-weighted policy
3. Evaluates correctness (if evaluator available)
4. Visualizes Q-values for both reference trajectories (from dataset) and extracted policy trajectories
"""
from __future__ import annotations

import argparse
import html as html_lib
import json
import os
import random
import shutil
import sys
import tempfile
import time
import traceback
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pyarrow.parquet as pq
import torch
import torch.multiprocessing as mp
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.cache_utils import DynamicCache


# ---------------------------------------------------------------------------
# Config
# ---------------------------------------------------------------------------

@dataclass
class Config:
    """Configuration for generation and visualization."""
    critic_path: str
    ref_path: str
    data_path: str
    out_dir: str  # Output directory for per-prompt HTML files

    num_prompts: int = 4
    num_samples: int = 4
    temperature: float = 1.0  # τ for Q-weighting

    min_p: float = 0.05
    max_new_tokens: int = 2048
    max_candidates_show: int = 15

    distribution_token_id: int = 151669
    label_column: str = "correct"
    max_length: int = 131072

    seed: int = 42
    dtype: str = "bfloat16"
    attn_implementation: str = "flash_attention_2"
    tokenizer_path: Optional[str] = None

    prompt_idx: Optional[List[int]] = None
    require_mixed_outcomes: bool = False
    min_correct_pct: float = 10.0
    min_incorrect_pct: float = 10.0

    # Show reference trajectories from dataset for comparison
    show_reference_trajectories: int = 2  # How many reference trajectories to show per prompt

    # Skip prompts with long responses
    max_avg_response_length: int = 2048

    # Data parallel
    dp_size: int = 0  # 0 = use all GPUs

    reward_col: str = "correct"

    # Posterior mode: π_new ∝ π_ref · Q instead of π_ref · exp(Q/τ)
    posterior_mode: bool = False

    # Computed fields (set by main before spawning)
    selected_prompt_ids: List[int] = None


# Qwen chat token ids
IM_START_TOKEN_ID = 151644
IM_END_TOKEN_ID = 151645
USER_TOKEN_ID = 872
ASSISTANT_TOKEN_ID = 77091
SYSTEM_TOKEN_ID = 8948
NEWLINE_TOKEN_ID = 198


def _torch_dtype(s: str) -> torch.dtype:
    s = (s or "").lower()
    if s == "float16":
        return torch.float16
    if s == "bfloat16":
        return torch.bfloat16
    if s == "float32":
        return torch.float32
    raise ValueError(f"bad dtype: {s}")


def _unwrap_model(m: Any) -> Any:
    m = m._orig_mod if hasattr(m, "_orig_mod") else m
    m = m.module if hasattr(m, "module") else m
    return m


def _get_lm_head(m: Any) -> Any:
    if hasattr(m, "lm_head"):
        return m.lm_head
    emb = m.get_output_embeddings() if hasattr(m, "get_output_embeddings") else None
    if emb is None:
        raise RuntimeError("no lm_head")
    return emb


def _get_base_model(m: Any) -> Any:
    for attr in ("model", "transformer", "base_model"):
        if hasattr(m, attr):
            return getattr(m, attr)
    return None


# ---------------------------------------------------------------------------
# Data structures
# ---------------------------------------------------------------------------

@dataclass
class Rollout:
    response_ids: List[int]
    reward: float
    prompt_token_ids: List[int] = None


@dataclass
class GeneratedTrajectory:
    """A trajectory generated by the extracted policy."""
    response_ids: List[int]
    # Per-token info for visualization
    # Each candidate tuple: (token_id, ref_prob, q_value, extracted_prob)
    token_candidates: List[List[Tuple[int, float, float, float]]]  # 4-tuple
    token_q_values: List[float]  # Q-value for the actually sampled token
    token_kl_divergences: List[float]  # KL(π_extracted || π_ref) at each step
    reward: Optional[float] = None  # If we can evaluate correctness


# ---------------------------------------------------------------------------
# KV Cache Helpers
# ---------------------------------------------------------------------------

class ReadOnlyCache(DynamicCache):
    """A cache wrapper that ignores update() calls, making it read-only."""

    def __init__(self, key_cache: List[torch.Tensor], value_cache: List[torch.Tensor]):
        super().__init__()
        self.key_cache = key_cache
        self.value_cache = value_cache
        if key_cache:
            self._seen_tokens = key_cache[0].shape[2]

    def update(
        self,
        key_states: torch.Tensor,
        value_states: torch.Tensor,
        layer_idx: int,
        cache_kwargs: Optional[Dict[str, Any]] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        if layer_idx < len(self.key_cache):
            full_k = torch.cat([self.key_cache[layer_idx], key_states], dim=2)
            full_v = torch.cat([self.value_cache[layer_idx], value_states], dim=2)
            return full_k, full_v
        return key_states, value_states

    def get_seq_length(self, layer_idx: int = 0) -> int:
        if layer_idx < len(self.key_cache):
            return self.key_cache[layer_idx].shape[2]
        return 0


def _slice_kv_cache(past_key_values: DynamicCache, end: int) -> DynamicCache:
    """Slice KV cache to [0:end] along the sequence dimension."""
    new_cache = DynamicCache()
    for layer_idx in range(len(past_key_values.key_cache)):
        k = past_key_values.key_cache[layer_idx]
        v = past_key_values.value_cache[layer_idx]
        if end <= 0:
            new_cache.update(k[:, :, :0, :], v[:, :, :0, :], layer_idx)
        else:
            new_cache.update(k[:, :, :end, :], v[:, :, :end, :], layer_idx)
    return new_cache


def _expand_kv_cache(past_key_values: DynamicCache, batch_size: int) -> ReadOnlyCache:
    """Expand KV cache batch dimension from 1 to batch_size via broadcasting."""
    expanded_keys = []
    expanded_values = []
    for layer_idx in range(len(past_key_values.key_cache)):
        k = past_key_values.key_cache[layer_idx]
        v = past_key_values.value_cache[layer_idx]
        expanded_keys.append(k.expand(batch_size, -1, -1, -1))
        expanded_values.append(v.expand(batch_size, -1, -1, -1))
    return ReadOnlyCache(expanded_keys, expanded_values)


# ---------------------------------------------------------------------------
# Sequence Packing (for critic context)
# ---------------------------------------------------------------------------

def create_tokenized_message(role: str, tokenized_content: List[int]) -> List[int]:
    role_token_id = {"system": SYSTEM_TOKEN_ID, "user": USER_TOKEN_ID, "assistant": ASSISTANT_TOKEN_ID}[role]
    content = list(tokenized_content)
    if content and content[-1] == IM_END_TOKEN_ID:
        content = content[:-1]
    return [IM_START_TOKEN_ID, role_token_id, NEWLINE_TOKEN_ID] + content + [IM_END_TOKEN_ID, NEWLINE_TOKEN_ID]


def _make_traj_block(traj_content: List[int]) -> Tuple[List[int], List[int], List[int]]:
    header = [IM_START_TOKEN_ID, ASSISTANT_TOKEN_ID, NEWLINE_TOKEN_ID]
    has_eos = bool(traj_content) and (traj_content[-1] == IM_END_TOKEN_ID)
    footer = [] if has_eos else [IM_END_TOKEN_ID]
    block = header + list(traj_content) + footer
    return header, footer, block


def _feedback_tokens(tokenizer: Any, reward: float, content_plus_footer_len: int) -> List[int]:
    s = f"Reward: {float(reward)}\nLength: {int(content_plus_footer_len)} tokens"
    return tokenizer.encode(s, add_special_tokens=False)


def _pack_context_for_critic(
    tokenizer: Any,
    prompt_text: str,
    context_rollouts: List[Rollout],
    max_length: int,
    rng: random.Random,
) -> List[int]:
    """
    Build a packed context sequence for the critic, matching training format exactly.

    Structure (same as train_in_context_critic.py):
        [user prompt] [ctx0 block] [ctx0 feedback] ... [ctxN block] [ctxN feedback] [target header]

    Where:
        - user prompt = create_tokenized_message("user", prompt_tokens)
        - ctx block = [IM_START, ASSISTANT, NEWLINE] + content + [IM_END if not present]
        - ctx feedback = create_tokenized_message("user", "Reward: X\nLength: Y tokens")
        - target header = [IM_START, ASSISTANT, NEWLINE]

    The target response tokens are appended after this prefix during generation.

    Returns:
        prefix_ids: The packed context ending with target header
    """
    # 1. User prompt (same as training)
    prompt_tokens = tokenizer.encode(prompt_text, add_special_tokens=False)
    prefix_ids = create_tokenized_message("user", prompt_tokens)

    # 2. Context rollouts with feedback (same as training)
    context_indices = list(range(len(context_rollouts)))
    rng.shuffle(context_indices)

    # Target header that will be added at the end
    target_header = [IM_START_TOKEN_ID, ASSISTANT_TOKEN_ID, NEWLINE_TOKEN_ID]

    for ridx in context_indices:
        r = context_rollouts[ridx]
        content = list(r.response_ids)

        # Build trajectory block: [IM_START, ASSISTANT, NEWLINE] + content + [IM_END if needed]
        header, footer_r, block = _make_traj_block(content)
        content_plus_footer_len = len(content) + len(footer_r)

        # Build feedback message: "Reward: X\nLength: Y tokens" (same format as training)
        fb_tokens = _feedback_tokens(tokenizer, r.reward, content_plus_footer_len)
        fb_msg = create_tokenized_message("user", fb_tokens)

        add_len = len(block) + len(fb_msg)
        # Reserve space for target header and generation
        if len(prefix_ids) + add_len + len(target_header) > max_length // 2:
            break

        prefix_ids.extend(block)
        prefix_ids.extend(fb_msg)

    # 3. Target header (assistant response start - content follows during generation)
    prefix_ids.extend(target_header)

    return prefix_ids


# ---------------------------------------------------------------------------
# Q-Weighted Generation
# ---------------------------------------------------------------------------

@torch.no_grad()
def _get_q_value_for_token(
    model: Any,
    hidden_state: torch.Tensor,  # [1, E] or [E]
    distribution_token_id: int,
    num_bins: int,
    num_length_bins: int,
    correct_reward_index: int,
) -> float:
    """Compute Q-value from hidden state."""
    if hidden_state.dim() == 1:
        hidden_state = hidden_state.unsqueeze(0)

    m = _unwrap_model(model)
    lm_head = _get_lm_head(m)
    w = lm_head.weight[distribution_token_id : distribution_token_id + num_bins]
    b = lm_head.bias[distribution_token_id : distribution_token_id + num_bins] if hasattr(lm_head, "bias") and lm_head.bias is not None else None

    logits = F.linear(hidden_state, w, b).float()
    num_reward_states = num_bins // num_length_bins
    logits_rs = logits.view(-1, num_reward_states, num_length_bins)
    logits_reward = torch.logsumexp(logits_rs, dim=2)
    probs_reward = torch.softmax(logits_reward, dim=1)

    return probs_reward[0, correct_reward_index].item()


@torch.no_grad()
def _generate_with_q_weighting(
    ref_model: Any,
    critic_model: Any,
    tokenizer: Any,
    prompt_token_ids: List[int],
    context_rollouts: List[Rollout],
    prompt_text: str,
    cfg: Config,
    device: torch.device,
    amp_dtype: torch.dtype,
    rng: random.Random,
    # Critic config
    num_bins: int,
    num_length_bins: int,
    correct_reward_index: int,
    # Progress tracking
    progress_prefix: str = "",
    progress_interval: int = 50,
    # Optional: exclude one rollout from context (to match training/reference eval structure)
    exclude_rollout_idx: Optional[int] = None,
) -> GeneratedTrajectory:
    """
    Generate a trajectory using Q-weighted sampling.

    At each step:
    1. Get candidates from reference model (min_p filtering)
    2. Compute Q-values for candidates using critic
    3. Sample from π_new ∝ π_ref · exp(Q/τ)
    """
    # Build context for critic (matching training format exactly)
    # Exclude one rollout from context to match training structure where
    # the target trajectory is NOT included in its own context
    if exclude_rollout_idx is not None and 0 <= exclude_rollout_idx < len(context_rollouts):
        ctx_rollouts = [r for i, r in enumerate(context_rollouts) if i != exclude_rollout_idx]
    else:
        ctx_rollouts = context_rollouts

    critic_prefix = _pack_context_for_critic(
        tokenizer, prompt_text, ctx_rollouts, cfg.max_length, rng
    )

    # Initialize sequences
    ref_seq = list(prompt_token_ids)  # For reference model (on-policy)
    critic_seq = list(critic_prefix)  # For critic (with context)

    generated_ids: List[int] = []
    token_candidates: List[List[Tuple[int, float, float, float]]] = []  # 4-tuple
    token_q_values: List[float] = []
    token_kl_divergences: List[float] = []

    # Progress tracking
    gen_start_time = time.time()
    last_progress_time = gen_start_time

    # Build initial KV caches
    ref_input = torch.tensor([ref_seq], dtype=torch.long, device=device)
    critic_input = torch.tensor([critic_seq], dtype=torch.long, device=device)

    with torch.autocast(device_type="cuda" if device.type == "cuda" else "cpu", dtype=amp_dtype):
        # Initial forward pass for reference model
        ref_out = ref_model(input_ids=ref_input, use_cache=True, return_dict=True)
        ref_kv = ref_out.past_key_values
        ref_logits = ref_out.logits[0, -1, :]  # [V]

        # Initial forward pass for critic model
        base = _get_base_model(_unwrap_model(critic_model))
        if base is None:
            critic_out = critic_model(
                input_ids=critic_input,
                use_cache=True,
                output_hidden_states=True,
                return_dict=True,
            )
            critic_hidden = critic_out.hidden_states[-1]
            critic_kv = critic_out.past_key_values
        else:
            critic_out = base(
                input_ids=critic_input,
                use_cache=True,
                return_dict=True,
            )
            critic_hidden = critic_out.last_hidden_state if hasattr(critic_out, "last_hidden_state") else critic_out.hidden_states[-1]
            critic_kv = critic_out.past_key_values

    del ref_input, critic_input

    for step in range(cfg.max_new_tokens):
        # Get candidates from reference model
        ref_log_probs = F.log_softmax(ref_logits.float(), dim=-1)
        ref_probs = ref_log_probs.exp()

        # Min-p filtering
        max_prob = ref_probs.max()
        threshold = cfg.min_p * max_prob
        mask = ref_probs >= threshold
        cand_indices = mask.nonzero(as_tuple=True)[0].tolist()

        if not cand_indices:
            # Fallback: take top token
            cand_indices = [ref_probs.argmax().item()]

        cand_ref_lps = ref_log_probs[cand_indices].tolist()

        # Get Q-values for candidates
        # Q-value is computed from hidden state AFTER feeding the candidate token
        # Process in batches to avoid OOM with many candidates
        num_cands = len(cand_indices)
        cand_q_values: List[float] = []

        # Batch size for candidate processing (tune based on GPU memory)
        CAND_BATCH_SIZE = 32

        sliced_kv = _slice_kv_cache(critic_kv, end=critic_kv.get_seq_length())

        for batch_start in range(0, num_cands, CAND_BATCH_SIZE):
            batch_end = min(batch_start + CAND_BATCH_SIZE, num_cands)
            batch_cand_indices = cand_indices[batch_start:batch_end]
            batch_size = len(batch_cand_indices)

            expanded_kv = _expand_kv_cache(sliced_kv, batch_size=batch_size)
            cand_input = torch.tensor(batch_cand_indices, dtype=torch.long, device=device).unsqueeze(1)  # [B, 1]

            with torch.autocast(device_type="cuda" if device.type == "cuda" else "cpu", dtype=amp_dtype):
                if base is None:
                    cand_out = critic_model(
                        input_ids=cand_input,
                        past_key_values=expanded_kv,
                        use_cache=False,
                        output_hidden_states=True,
                        return_dict=True,
                    )
                    cand_hidden = cand_out.hidden_states[-1]
                else:
                    cand_out = base(
                        input_ids=cand_input,
                        past_key_values=expanded_kv,
                        use_cache=False,
                        return_dict=True,
                    )
                    cand_hidden = cand_out.last_hidden_state if hasattr(cand_out, "last_hidden_state") else cand_out.hidden_states[-1]

            # Extract Q-values from hidden states after each candidate
            for i in range(batch_size):
                h = cand_hidden[i, 0, :]  # [E]
                q = _get_q_value_for_token(
                    critic_model, h, cfg.distribution_token_id,
                    num_bins, num_length_bins, correct_reward_index
                )
                cand_q_values.append(q)

            del expanded_kv, cand_input, cand_out, cand_hidden

        del sliced_kv

        # Compute weighted distribution
        q_tensor = torch.tensor(cand_q_values, device=device)
        ref_probs_cand = torch.tensor([ref_probs[i].item() for i in cand_indices], device=device)

        if cfg.posterior_mode:
            # Posterior mode: π_new ∝ π_ref · Q (Bayesian posterior P(a|s,success))
            weights = ref_probs_cand * q_tensor
            weights = weights / weights.sum()
        else:
            # Exp mode: π_new ∝ π_ref · exp(Q/τ)
            # Use log-space computation for numerical stability with small τ
            ref_lps_cand = torch.tensor([ref_log_probs[i].item() for i in cand_indices], device=device)
            if cfg.temperature > 0:
                # log(π_new) = log(π_ref) + Q/τ, then softmax to normalize
                log_weights = ref_lps_cand + q_tensor / cfg.temperature
                weights = F.softmax(log_weights, dim=0)
            else:
                # Temperature = 0: greedy w.r.t. Q-value
                weights = torch.zeros_like(ref_lps_cand)
                weights[q_tensor.argmax()] = 1.0

        # weights = extracted policy probabilities (normalized)
        extracted_probs = weights.tolist()

        # Compute KL divergence: KL(π_extracted || π_ref) = Σ π_extracted * log(π_extracted / π_ref)
        # Only over the candidate set (min-p filtered tokens)
        eps = 1e-10
        kl_div = 0.0
        for i in range(num_cands):
            p_ext = extracted_probs[i]
            p_ref = ref_probs_cand[i].item()
            if p_ext > eps and p_ref > eps:
                kl_div += p_ext * np.log(p_ext / p_ref)
        token_kl_divergences.append(kl_div)

        # Sample
        sampled_idx = torch.multinomial(weights, 1).item()
        sampled_token = cand_indices[sampled_idx]
        sampled_q = cand_q_values[sampled_idx]

        # Store candidates as 4-tuple: (token_id, ref_prob, q_value, extracted_prob)
        # Sorted by Q-value
        candidates = [
            (cand_indices[i], ref_probs_cand[i].item(), cand_q_values[i], extracted_probs[i])
            for i in range(num_cands)
        ]
        candidates.sort(key=lambda x: -x[2])  # Sort by Q-value descending
        token_candidates.append(candidates)
        token_q_values.append(sampled_q)
        generated_ids.append(sampled_token)

        # Progress output
        current_time = time.time()
        if (step + 1) % progress_interval == 0 or current_time - last_progress_time >= 5.0:
            elapsed = current_time - gen_start_time
            toks_per_sec = (step + 1) / elapsed if elapsed > 0 else 0
            print(
                f"\r{progress_prefix}[{step+1}/{cfg.max_new_tokens}] "
                f"{toks_per_sec:.1f} tok/s, q={sampled_q:.3f}, "
                f"cands={num_cands}",
                end="", flush=True
            )
            last_progress_time = current_time

        # Check for EOS
        if sampled_token == IM_END_TOKEN_ID or sampled_token == tokenizer.eos_token_id:
            break

        # Update sequences and KV caches
        next_token = torch.tensor([[sampled_token]], dtype=torch.long, device=device)

        with torch.autocast(device_type="cuda" if device.type == "cuda" else "cpu", dtype=amp_dtype):
            # Update reference model
            ref_out = ref_model(input_ids=next_token, past_key_values=ref_kv, use_cache=True, return_dict=True)
            ref_kv = ref_out.past_key_values
            ref_logits = ref_out.logits[0, -1, :]

            # Update critic model
            if base is None:
                critic_out = critic_model(
                    input_ids=next_token,
                    past_key_values=critic_kv,
                    use_cache=True,
                    output_hidden_states=True,
                    return_dict=True,
                )
                critic_hidden = critic_out.hidden_states[-1]
                critic_kv = critic_out.past_key_values
            else:
                critic_out = base(
                    input_ids=next_token,
                    past_key_values=critic_kv,
                    use_cache=True,
                    return_dict=True,
                )
                critic_hidden = critic_out.last_hidden_state if hasattr(critic_out, "last_hidden_state") else critic_out.hidden_states[-1]
                critic_kv = critic_out.past_key_values

        del next_token

    # Clear progress line - show final Q value
    if generated_ids:
        final_q = token_q_values[-1] if token_q_values else 0.0
        elapsed = time.time() - gen_start_time
        toks_per_sec = len(generated_ids) / elapsed if elapsed > 0 else 0
        print(
            f"\r{progress_prefix}[{len(generated_ids)} toks] "
            f"{toks_per_sec:.1f} tok/s, final_q={final_q:.3f}{' ' * 10}",
            flush=True
        )

    # Cleanup
    del ref_kv, critic_kv, ref_logits, critic_hidden
    if device.type == "cuda":
        torch.cuda.empty_cache()

    return GeneratedTrajectory(
        response_ids=generated_ids,
        token_candidates=token_candidates,
        token_q_values=token_q_values,
        token_kl_divergences=token_kl_divergences,
        reward=None,  # Will be set later if we can evaluate
    )


# ---------------------------------------------------------------------------
# Q-value evaluation for existing trajectories
# ---------------------------------------------------------------------------

@torch.no_grad()
def _evaluate_trajectory_q_values(
    ref_model: Any,
    critic_model: Any,
    tokenizer: Any,
    rollout: Rollout,
    context_rollouts: List[Rollout],
    prompt_text: str,
    cfg: Config,
    device: torch.device,
    amp_dtype: torch.dtype,
    rng: random.Random,
    num_bins: int,
    num_length_bins: int,
    correct_reward_index: int,
) -> Tuple[List[float], List[List[Tuple[int, float, float, float]]], List[float]]:
    """
    Evaluate Q-values for an existing trajectory.

    Returns:
        token_q_values: Q-value for each token
        token_candidates: [(token_id, ref_prob, q_value, extracted_prob), ...] for each position
        token_kl_divergences: KL(π_extracted || π_ref) at each step
    """
    response_ids = list(rollout.response_ids)
    prompt_ids = list(rollout.prompt_token_ids)

    if not response_ids:
        return [], [], []

    # Build context for critic (matching training format exactly)
    # Filter out the target rollout from context (same as visualization script)
    ctx_rollouts = [r for r in context_rollouts if r is not rollout]
    critic_prefix = _pack_context_for_critic(
        tokenizer, prompt_text, ctx_rollouts, cfg.max_length, rng
    )

    # Reference model: on-policy sequence
    ref_seq = prompt_ids + response_ids
    ref_input = torch.tensor([ref_seq], dtype=torch.long, device=device)

    # Critic model: packed sequence with context + target response
    critic_seq = critic_prefix + response_ids
    # Add EOS if not present
    if response_ids[-1] != IM_END_TOKEN_ID:
        critic_seq.append(IM_END_TOKEN_ID)
    critic_input = torch.tensor([critic_seq], dtype=torch.long, device=device)

    with torch.autocast(device_type="cuda" if device.type == "cuda" else "cpu", dtype=amp_dtype):
        # Reference model forward
        ref_out = ref_model(input_ids=ref_input, use_cache=False, return_dict=True)
        ref_logits = ref_out.logits  # [1, S, V]

        # Critic model forward
        base = _get_base_model(_unwrap_model(critic_model))
        if base is None:
            critic_out = critic_model(
                input_ids=critic_input,
                use_cache=False,
                output_hidden_states=True,
                return_dict=True,
            )
            critic_hidden = critic_out.hidden_states[-1]
        else:
            critic_out = base(
                input_ids=critic_input,
                use_cache=False,
                return_dict=True,
            )
            critic_hidden = critic_out.last_hidden_state if hasattr(critic_out, "last_hidden_state") else critic_out.hidden_states[-1]

    # Extract Q-values and candidates for each position
    token_q_values: List[float] = []
    token_candidates: List[List[Tuple[int, float, float, float]]] = []  # 4-tuple
    token_kl_divergences: List[float] = []

    m = _unwrap_model(critic_model)
    lm_head = _get_lm_head(m)
    w = lm_head.weight[cfg.distribution_token_id : cfg.distribution_token_id + num_bins]
    b = lm_head.bias[cfg.distribution_token_id : cfg.distribution_token_id + num_bins] if hasattr(lm_head, "bias") and lm_head.bias is not None else None
    num_reward_states = num_bins // num_length_bins

    for i, tid in enumerate(response_ids):
        # Reference position: position before this token
        ref_pos = len(prompt_ids) + i - 1
        if ref_pos < 0:
            ref_pos = 0

        # Get reference logprobs
        pos_logits = ref_logits[0, ref_pos, :]
        ref_log_probs_all = F.log_softmax(pos_logits.float(), dim=-1)
        ref_probs_all = ref_log_probs_all.exp()

        # Min-p filtering
        max_prob = ref_probs_all.max()
        threshold = cfg.min_p * max_prob
        mask = ref_probs_all >= threshold
        cand_indices = mask.nonzero(as_tuple=True)[0].tolist()

        # Always include actual token
        if tid not in cand_indices:
            cand_indices.append(tid)

        # Critic position: position of this token in critic sequence
        critic_pos = len(critic_prefix) + i

        # Get Q-value for actual token
        h = critic_hidden[0, critic_pos, :]
        logits_h = F.linear(h, w, b).float()
        logits_rs = logits_h.view(num_reward_states, num_length_bins)
        logits_reward = torch.logsumexp(logits_rs, dim=1)
        probs_reward = torch.softmax(logits_reward, dim=0)
        actual_q = probs_reward[correct_reward_index].item()

        token_q_values.append(actual_q)

        # Get reference probs for candidates
        ref_probs_cand = [ref_probs_all[cid].item() for cid in cand_indices]

        # For reference trajectories, we approximate Q-values (expensive to compute all)
        # Use actual Q for the actual token, neutral Q=0.5 for others
        cand_q_values = []
        for cid in cand_indices:
            if cid == tid:
                cand_q_values.append(actual_q)
            else:
                cand_q_values.append(0.5)  # Neutral Q for others (approximation)

        # Compute extracted policy probabilities
        q_tensor = torch.tensor(cand_q_values, device=device)
        ref_probs_tensor = torch.tensor(ref_probs_cand, device=device)

        if cfg.posterior_mode:
            weights = ref_probs_tensor * q_tensor
            weights = weights / (weights.sum() + 1e-10)
        else:
            ref_lps_cand = torch.tensor([ref_log_probs_all[cid].item() for cid in cand_indices], device=device)
            if cfg.temperature > 0:
                log_weights = ref_lps_cand + q_tensor / cfg.temperature
                weights = F.softmax(log_weights, dim=0)
            else:
                weights = torch.zeros_like(ref_probs_tensor)
                weights[q_tensor.argmax()] = 1.0

        extracted_probs = weights.tolist()

        # Compute KL divergence
        eps = 1e-10
        kl_div = 0.0
        for j in range(len(cand_indices)):
            p_ext = extracted_probs[j]
            p_ref = ref_probs_cand[j]
            if p_ext > eps and p_ref > eps:
                kl_div += p_ext * np.log(p_ext / p_ref)
        token_kl_divergences.append(kl_div)

        # Build candidates as 4-tuple: (token_id, ref_prob, q_value, extracted_prob)
        candidates = [
            (cand_indices[j], ref_probs_cand[j], cand_q_values[j], extracted_probs[j])
            for j in range(len(cand_indices))
        ]
        candidates.sort(key=lambda x: -x[2])  # Sort by Q-value descending
        token_candidates.append(candidates)

    del ref_input, critic_input, ref_out, critic_out, ref_logits, critic_hidden
    if device.type == "cuda":
        torch.cuda.empty_cache()

    return token_q_values, token_candidates, token_kl_divergences


# ---------------------------------------------------------------------------
# HTML Generation
# ---------------------------------------------------------------------------

def _reward_rgb(r: float) -> str:
    """Q-value color: red (0) to green (1)."""
    x = max(0.0, min(1.0, float(r)))
    rr = int((1.0 - x) * 255.0)
    gg = int(x * 255.0)
    return f"rgb({rr},{gg},0)"


def _kl_rgb(kl: float, max_kl: float = 2.0) -> str:
    """KL divergence color: white (0) to purple (high KL).

    Args:
        kl: KL divergence value
        max_kl: Maximum KL value for normalization (default 2.0)
    """
    x = max(0.0, min(1.0, kl / max_kl))
    # White (255,255,255) to purple (128, 0, 128)
    rr = int(255 - x * 127)
    gg = int(255 - x * 255)
    bb = int(255 - x * 127)
    return f"rgb({rr},{gg},{bb})"


def _decode_token(tokenizer: Any, tid: int, cache: Dict[int, str]) -> str:
    if tid in cache:
        return cache[tid]
    s = tokenizer.decode([int(tid)], clean_up_tokenization_spaces=False, skip_special_tokens=False)
    if s == "":
        s = f"<{int(tid)}>"
    cache[tid] = s
    return s


def _html_header(title: str) -> str:
    t = html_lib.escape(title)
    return f"""<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>{t}</title>
<style>
body {{ font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif; margin: 16px; }}
h1,h2,h3 {{ margin: 12px 0 8px; }}
pre {{ background: #f6f6f6; padding: 10px; border: 1px solid #ddd; overflow-x: auto; white-space: pre-wrap; }}
.meta {{ font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", monospace; font-size: 12px; background: #fbfbfb; border: 1px solid #ddd; padding: 10px; }}
.grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(360px, 1fr)); gap: 12px; align-items: start; }}
.card {{ border: 1px solid #ddd; padding: 10px; }}
.card.extracted {{ border-color: #4a90d9; border-width: 2px; }}
.card.reference {{ border-color: #888; }}
.card .hdr {{ font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", monospace; font-size: 12px; margin-bottom: 6px; }}
.toks {{ white-space: pre-wrap; font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", monospace; font-size: 12px; line-height: 1.4; }}
.tok {{ padding: 0 1px; border-radius: 2px; cursor: pointer; position: relative; }}
.tok:hover {{ outline: 2px solid #333; }}
.legend {{ display:flex; align-items:center; gap:10px; margin: 10px 0; flex-wrap: wrap; }}
.bar {{ width: 180px; height: 14px; border: 1px solid #bbb; }}
.bar.q {{ background: linear-gradient(to right, rgb(255,0,0), rgb(0,255,0)); }}
.bar.kl {{ background: linear-gradient(to right, rgb(255,255,255), rgb(128,0,128)); }}
.small {{ font-size: 12px; color: #333; }}
.tag {{ display: inline-block; padding: 2px 6px; border-radius: 3px; font-size: 11px; margin-right: 6px; }}
.tag.extracted {{ background: #4a90d9; color: white; }}
.tag.reference {{ background: #888; color: white; }}
.stats {{ background: #f0f8ff; border: 1px solid #4a90d9; padding: 10px; margin: 10px 0; }}
hr {{ border: 0; border-top: 1px solid #ddd; margin: 16px 0; }}

/* Dual row visualization */
.traj-rows {{ margin: 4px 0; }}
.traj-row {{ margin: 2px 0; }}
.row-label {{ font-size: 10px; color: #666; display: inline-block; width: 24px; text-align: right; margin-right: 4px; }}

/* Tooltip styles */
.tooltip {{
  display: none;
  position: absolute;
  bottom: 100%;
  left: 50%;
  transform: translateX(-50%);
  background: #222;
  color: #fff;
  padding: 8px;
  border-radius: 4px;
  font-size: 11px;
  white-space: nowrap;
  z-index: 1000;
  max-height: 300px;
  overflow-y: auto;
}}
.tok:hover .tooltip {{ display: block; }}
.tooltip table {{ border-collapse: collapse; }}
.tooltip th, .tooltip td {{ padding: 2px 6px; text-align: left; border-bottom: 1px solid #444; }}
.tooltip th {{ color: #aaa; }}
.tooltip .actual {{ background: #363; }}
</style>
</head>
<body>
<h1>{t}</h1>
<div class="legend">
  <div class="bar q"></div>
  <div class="small">Q-value: 0 (red) → 1 (green)</div>
  <div class="bar kl"></div>
  <div class="small">KL: 0 (white) → high (purple)</div>
  <span class="tag extracted">Extracted Policy</span>
  <span class="tag reference">Reference (Dataset)</span>
</div>
"""


def _html_footer() -> str:
    return "</body>\n</html>\n"


def _make_tooltip_html(
    tokenizer: Any,
    actual_tid: int,
    candidates: List[Tuple[int, float, float, float]],  # 4-tuple
    decode_cache: Dict[int, str],
    max_show: int = 15,
    kl_value: Optional[float] = None,
) -> str:
    """Create tooltip HTML showing token candidates with probabilities.

    Each candidate is (token_id, ref_prob, q_value, extracted_prob).
    """
    lines = ['<div class="tooltip"><table>']

    # Show KL value if provided
    if kl_value is not None:
        lines.append(f"<tr><td colspan='4' style='text-align:center; color:#aaa;'>KL: {kl_value:.4f}</td></tr>")

    lines.append("<tr><th>Token</th><th>π_ref</th><th>Q</th><th>π_ext</th></tr>")

    for tid, ref_prob, q_val, ext_prob in candidates[:max_show]:
        tok_str = html_lib.escape(_decode_token(tokenizer, tid, decode_cache))
        is_actual = (tid == actual_tid)
        row_class = ' class="actual"' if is_actual else ""
        marker = " *" if is_actual else ""
        lines.append(
            f"<tr{row_class}><td>{tok_str}{marker}</td>"
            f"<td>{ref_prob:.3f}</td><td>{q_val:.3f}</td><td>{ext_prob:.3f}</td></tr>"
        )

    if len(candidates) > max_show:
        lines.append(f"<tr><td colspan='4'>... +{len(candidates) - max_show} more</td></tr>")

    lines.append("</table></div>")
    return "".join(lines)


# ---------------------------------------------------------------------------
# Data Loading
# ---------------------------------------------------------------------------

def _has_mixed_outcomes(
    rollouts: List[Rollout],
    threshold: float = 0.5,
    min_correct_pct: float = 0.0,
    min_incorrect_pct: float = 0.0,
) -> bool:
    if len(rollouts) < 2:
        return False
    rewards = [r.reward for r in rollouts]
    num_correct = sum(1 for r in rewards if r >= threshold)
    num_incorrect = len(rewards) - num_correct
    correct_pct = 100.0 * num_correct / len(rewards)
    incorrect_pct = 100.0 * num_incorrect / len(rewards)
    return correct_pct >= min_correct_pct and incorrect_pct >= min_incorrect_pct


def _avg_response_length(rollouts: List[Rollout]) -> float:
    """Compute average response length across rollouts."""
    if not rollouts:
        return 0.0
    return sum(len(r.response_ids) for r in rollouts) / len(rollouts)


def _load_prompt_groups(cfg: Config) -> Dict[int, Dict[str, Any]]:
    table = pq.read_table(cfg.data_path, columns=["prompt_idx", "prompt", "prompt_token_ids", "output_token_ids", cfg.reward_col])
    df = table.to_pandas()

    if cfg.reward_col == "correct":
        df["correct"] = df["correct"].astype(float)

    prompt_groups: Dict[int, Dict[str, Any]] = {}
    grouped = df.groupby("prompt_idx", sort=True)
    for pid, group in grouped:
        rows = group.to_dict("records")
        if not rows:
            continue
        prompt_text = rows[0]["prompt"]
        ro: List[Rollout] = []
        for r in rows:
            toks = r["output_token_ids"]
            if hasattr(toks, "tolist"):
                toks = toks.tolist()
            prompt_toks = r["prompt_token_ids"]
            if hasattr(prompt_toks, "tolist"):
                prompt_toks = prompt_toks.tolist()
            ro.append(Rollout(
                response_ids=list(toks),
                reward=float(r[cfg.reward_col]),
                prompt_token_ids=list(prompt_toks),
            ))
        prompt_groups[int(pid)] = {"prompt": prompt_text, "rollouts": ro}

    return prompt_groups


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Generate trajectories using Q-weighted policy")
    p.add_argument("--critic_path", required=True, help="Path to critic model")
    p.add_argument("--ref_path", default="Qwen/Qwen3-1.7B", help="Path to reference model")
    p.add_argument("--data_path", required=True, help="Path to parquet data")
    p.add_argument("--out_dir", required=True, help="Output directory for per-prompt HTML files")

    p.add_argument("--num_prompts", type=int, default=4, help="Number of prompts to process")
    p.add_argument("--num_samples", type=int, default=4, help="Samples per prompt from extracted policy")
    p.add_argument("--temperature", type=float, default=1.0, help="Temperature τ for Q-weighting")

    p.add_argument("--min_p", type=float, default=0.01, help="Min-p threshold for candidate filtering")
    p.add_argument("--max_new_tokens", type=int, default=2048, help="Max tokens to generate")
    p.add_argument("--max_candidates_show", type=int, default=15, help="Max candidates to show in tooltip")

    p.add_argument("--distribution_token_id", type=int, default=151669)
    p.add_argument("--label_column", choices=["correct", "value"], default="correct")
    p.add_argument("--max_length", type=int, default=131072)

    p.add_argument("--seed", type=int, default=42)
    p.add_argument("--dtype", type=str, choices=["float16", "bfloat16", "float32"], default="bfloat16")
    p.add_argument("--attn_implementation", type=str, default="flash_attention_2")
    p.add_argument("--tokenizer_path", type=str, default=None)

    p.add_argument("--prompt_idx", type=int, nargs="*", default=None)
    p.add_argument("--require_mixed_outcomes", action="store_true")
    p.add_argument("--min_correct_pct", type=float, default=10.0)
    p.add_argument("--min_incorrect_pct", type=float, default=10.0)

    p.add_argument("--show_reference_trajectories", type=int, default=4,
                   help="Number of reference trajectories to show per prompt")
    p.add_argument("--max_avg_response_length", type=int, default=1536,
                   help="Skip prompts with average response length greater than this")
    p.add_argument("--dp_size", type=int, default=0,
                   help="Data parallel size (0=use all GPUs)")
    p.add_argument("--posterior_mode", action="store_true",
                   help="Use posterior mode: π ∝ π_ref · Q instead of π_ref · exp(Q/τ)")

    return p.parse_args()


def _print_progress(msg: str, end: str = "\n") -> None:
    timestamp = time.strftime("%H:%M:%S")
    print(f"[{timestamp}] {msg}", end=end, flush=True)


def _worker(rank: int, world: int, cfg: Config, fragment_dir: str) -> None:
    """Worker process that handles a shard of prompts on a specific GPU."""
    device = torch.device(f"cuda:{rank}" if torch.cuda.is_available() else "cpu")
    if device.type == "cuda":
        torch.cuda.set_device(device)

    torch.manual_seed(int(cfg.seed) + rank)
    amp_dtype = _torch_dtype(cfg.dtype)

    _print_progress(f"[Rank {rank}] Loading models on {device}...")

    model_kwargs: Dict[str, Any] = {"torch_dtype": amp_dtype, "trust_remote_code": True}
    attn_impl = cfg.attn_implementation
    if device.type != "cuda" and attn_impl == "flash_attention_2":
        attn_impl = "sdpa"
    if attn_impl and attn_impl != "auto":
        model_kwargs["attn_implementation"] = attn_impl

    critic_model = AutoModelForCausalLM.from_pretrained(cfg.critic_path, **model_kwargs)
    critic_model.to(device)
    critic_model.eval()

    ref_model = AutoModelForCausalLM.from_pretrained(cfg.ref_path, **model_kwargs)
    ref_model.to(device)
    ref_model.eval()

    tok_src = cfg.tokenizer_path if cfg.tokenizer_path else cfg.ref_path
    tokenizer = AutoTokenizer.from_pretrained(tok_src, trust_remote_code=True)
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    _print_progress(f"[Rank {rank}] Models loaded, loading data...")

    # Load data
    prompt_groups = _load_prompt_groups(cfg)

    # Get assigned prompts for this worker
    all_prompt_ids = cfg.selected_prompt_ids
    my_prompt_ids = [pid for i, pid in enumerate(all_prompt_ids) if i % world == rank]

    if not my_prompt_ids:
        _print_progress(f"[Rank {rank}] No prompts assigned, exiting.")
        fragment_path = os.path.join(fragment_dir, f"fragment_{rank:04d}.html")
        with open(fragment_path, "w", encoding="utf-8") as f:
            f.write("")
        return

    _print_progress(f"[Rank {rank}] Processing {len(my_prompt_ids)} prompts...")

    # Critic config
    if cfg.reward_col == "correct":
        reward_values = [0.0, 1.0]
    else:
        reward_values = [0.0, 0.1666667, 0.3333333, 0.5, 0.6666667, 0.8333333, 1.0]

    length_bins = [0, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]
    num_length_bins = len(length_bins) - 1
    num_reward_states = len(reward_values)
    num_bins = num_length_bins * num_reward_states
    correct_reward_index = int(max(range(len(reward_values)), key=lambda i: reward_values[i]))

    decode_cache: Dict[int, str] = {}
    parts: List[str] = []

    # Stats
    total_extracted = 0
    total_reference = 0
    prompt_times: List[float] = []

    # Performance tracking (for summary)
    prompt_stats: List[Dict[str, Any]] = []

    for local_i, pid in enumerate(my_prompt_ids):
        prompt_start = time.time()

        # Find global index for display
        global_i = all_prompt_ids.index(pid) + 1

        grp = prompt_groups.get(int(pid), None)
        if grp is None:
            continue

        prompt_text = str(grp["prompt"])
        all_rollouts: List[Rollout] = list(grp["rollouts"])

        # ETA calculation
        if prompt_times:
            avg_prompt_time = np.mean(prompt_times)
            remaining_prompts = len(my_prompt_ids) - local_i
            eta_seconds = avg_prompt_time * remaining_prompts
            eta_str = f", ETA: {eta_seconds/60:.1f}min" if eta_seconds >= 60 else f", ETA: {eta_seconds:.0f}s"
        else:
            eta_str = ""

        _print_progress(f"[Rank {rank}] Prompt {local_i+1}/{len(my_prompt_ids)} (global {global_i}, idx={pid}){eta_str}")

        parts.append("<hr>")
        parts.append(f"<h2>Prompt {global_i} | prompt_idx {int(pid)}</h2>")
        parts.append("<h3>Prompt</h3>")
        parts.append(f"<pre>{html_lib.escape(prompt_text)}</pre>")

        # Stats for this prompt
        extracted_final_q: List[float] = []
        reference_gt: List[float] = []
        reference_final_q: List[float] = []

        # Store trajectory data for two-pass rendering (need max_kl first)
        extracted_trajs: List[Tuple[int, GeneratedTrajectory]] = []  # (sample_i, traj)
        reference_trajs: List[Tuple[int, Rollout, List[float], List[List[Tuple[int, float, float, float]]], List[float]]] = []
        # reference_trajs: (ref_i, rollout, q_values, candidates, kl_values)

        # Pass 1: Generate all trajectories and collect KL values
        for sample_i in range(cfg.num_samples):
            # Exclude one rollout from context to match training structure.
            # During training, the target trajectory is NOT included in its own context.
            # We use sample_i % len(all_rollouts) to vary which rollout is excluded,
            # making the context match what reference evaluation would see for that rollout.
            exclude_idx = sample_i % len(all_rollouts)

            # Use the SAME RNG seed as reference evaluation for this excluded rollout.
            # This ensures the context shuffling is identical.
            # Reference eval uses: (cfg.seed * 1000003 + pid * 1009 + ref_i * 7)
            seed_i = (cfg.seed * 1000003 + pid * 1009 + exclude_idx * 7) & 0xFFFFFFFF
            rng = random.Random(seed_i)
            context_rollouts = all_rollouts
            progress_prefix = f"[Rank {rank}] Sample {sample_i+1}/{cfg.num_samples} (excl ref {exclude_idx}): "

            traj = _generate_with_q_weighting(
                ref_model=ref_model,
                critic_model=critic_model,
                tokenizer=tokenizer,
                prompt_token_ids=all_rollouts[0].prompt_token_ids,
                context_rollouts=context_rollouts,
                prompt_text=prompt_text,
                cfg=cfg,
                device=device,
                amp_dtype=amp_dtype,
                rng=rng,
                num_bins=num_bins,
                num_length_bins=num_length_bins,
                correct_reward_index=correct_reward_index,
                progress_prefix=progress_prefix,
                exclude_rollout_idx=exclude_idx,
            )

            final_q = traj.token_q_values[-1] if traj.token_q_values else 0.0
            total_extracted += 1
            extracted_final_q.append(final_q)
            extracted_trajs.append((sample_i, traj))

        # Evaluate reference trajectories
        num_ref_to_show = min(cfg.show_reference_trajectories, len(all_rollouts))
        for ref_i in range(num_ref_to_show):
            ref_start = time.time()
            rollout = all_rollouts[ref_i]
            print(f"[Rank {rank}] Reference {ref_i+1}/{num_ref_to_show}: evaluating {len(rollout.response_ids)} tokens...", end="", flush=True)
            seed_i = (cfg.seed * 1000003 + pid * 1009 + ref_i * 7) & 0xFFFFFFFF
            rng = random.Random(seed_i)

            q_values, candidates, kl_values = _evaluate_trajectory_q_values(
                ref_model=ref_model,
                critic_model=critic_model,
                tokenizer=tokenizer,
                rollout=rollout,
                context_rollouts=all_rollouts,
                prompt_text=prompt_text,
                cfg=cfg,
                device=device,
                amp_dtype=amp_dtype,
                rng=rng,
                num_bins=num_bins,
                num_length_bins=num_length_bins,
                correct_reward_index=correct_reward_index,
            )

            ref_time = time.time() - ref_start
            total_reference += 1
            final_q = q_values[-1] if q_values else 0.0
            print(f" done in {ref_time:.1f}s, final_q={final_q:.3f}", flush=True)
            reference_gt.append(rollout.reward)
            reference_final_q.append(final_q)
            reference_trajs.append((ref_i, rollout, q_values, candidates, kl_values))

        # Compute max KL across all trajectories for this prompt
        all_kl_values: List[float] = []
        for _, traj in extracted_trajs:
            all_kl_values.extend(traj.token_kl_divergences)
        for _, _, _, _, kl_values in reference_trajs:
            all_kl_values.extend(kl_values)
        max_kl = max(all_kl_values) if all_kl_values else 1.0
        max_kl = max(max_kl, 0.01)  # Avoid division issues

        # Pass 2: Build HTML with normalized KL colors
        if cfg.posterior_mode:
            parts.append("<h3>Extracted Policy (Posterior)</h3>")
        else:
            parts.append(f"<h3>Extracted Policy (τ={cfg.temperature})</h3>")
        parts.append('<div class="grid">')

        for sample_i, traj in extracted_trajs:
            final_q = traj.token_q_values[-1] if traj.token_q_values else 0.0
            avg_kl = np.mean(traj.token_kl_divergences) if traj.token_kl_divergences else 0.0

            q_spans: List[str] = []
            kl_spans: List[str] = []
            for idx, tid in enumerate(traj.response_ids):
                q_val = traj.token_q_values[idx] if idx < len(traj.token_q_values) else 0.0
                kl_val = traj.token_kl_divergences[idx] if idx < len(traj.token_kl_divergences) else 0.0
                candidates = traj.token_candidates[idx] if idx < len(traj.token_candidates) else []

                tok_str = html_lib.escape(_decode_token(tokenizer, int(tid), decode_cache))
                tooltip_html = _make_tooltip_html(tokenizer, tid, candidates, decode_cache, cfg.max_candidates_show, kl_value=kl_val)

                q_spans.append(
                    f'<span class="tok" style="background-color:{_reward_rgb(q_val)}">'
                    f'{tok_str}{tooltip_html}</span>'
                )
                kl_spans.append(
                    f'<span class="tok" style="background-color:{_kl_rgb(kl_val, max_kl)}">'
                    f'{tok_str}{tooltip_html}</span>'
                )

            parts.append('<div class="card extracted">')
            parts.append('<div class="hdr">')
            parts.append(
                html_lib.escape(
                    f"sample {sample_i} | final_q {final_q:.4f} | avg_kl {avg_kl:.3f} | "
                    f"tok {len(traj.response_ids)}"
                )
            )
            parts.append("</div>")
            parts.append('<div class="traj-rows">')
            parts.append(f'<div class="traj-row"><span class="row-label">Q:</span><span class="toks">{"".join(q_spans)}</span></div>')
            parts.append(f'<div class="traj-row"><span class="row-label">KL:</span><span class="toks">{"".join(kl_spans)}</span></div>')
            parts.append("</div>")
            parts.append("</div>")

        parts.append("</div>")

        # Reference trajectories
        if reference_trajs:
            parts.append(f"<h3>Reference Trajectories (from dataset)</h3>")
            parts.append('<div class="grid">')

            for ref_i, rollout, q_values, candidates, kl_values in reference_trajs:
                final_q = q_values[-1] if q_values else 0.0
                avg_kl = np.mean(kl_values) if kl_values else 0.0

                q_spans: List[str] = []
                kl_spans: List[str] = []
                for idx, tid in enumerate(rollout.response_ids):
                    q_val = q_values[idx] if idx < len(q_values) else 0.0
                    kl_val = kl_values[idx] if idx < len(kl_values) else 0.0
                    cands = candidates[idx] if idx < len(candidates) else []

                    tok_str = html_lib.escape(_decode_token(tokenizer, int(tid), decode_cache))
                    tooltip_html = _make_tooltip_html(tokenizer, tid, cands, decode_cache, cfg.max_candidates_show, kl_value=kl_val)

                    q_spans.append(
                        f'<span class="tok" style="background-color:{_reward_rgb(q_val)}">'
                        f'{tok_str}{tooltip_html}</span>'
                    )
                    kl_spans.append(
                        f'<span class="tok" style="background-color:{_kl_rgb(kl_val, max_kl)}">'
                        f'{tok_str}{tooltip_html}</span>'
                    )

                parts.append('<div class="card reference">')
                parts.append('<div class="hdr">')
                parts.append(
                    html_lib.escape(
                        f"ref {ref_i} | gt {rollout.reward:.4f} | final_q {final_q:.4f} | avg_kl {avg_kl:.3f} | "
                        f"tok {len(rollout.response_ids)}"
                    )
                )
                parts.append("</div>")
                parts.append('<div class="traj-rows">')
                parts.append(f'<div class="traj-row"><span class="row-label">Q:</span><span class="toks">{"".join(q_spans)}</span></div>')
                parts.append(f'<div class="traj-row"><span class="row-label">KL:</span><span class="toks">{"".join(kl_spans)}</span></div>')
                parts.append("</div>")
                parts.append("</div>")

            parts.append("</div>")

        # Collect stats for this prompt
        prompt_stats.append({
            'prompt_idx': pid,
            'extracted_final_q': extracted_final_q,
            'reference_gt': reference_gt,
            'reference_final_q': reference_final_q,
        })

        # Record prompt time
        prompt_time = time.time() - prompt_start
        prompt_times.append(prompt_time)
        _print_progress(f"[Rank {rank}] Prompt {local_i+1} completed in {prompt_time:.1f}s")

    # Write fragment to file
    fragment_path = os.path.join(fragment_dir, f"fragment_{rank:04d}.html")
    with open(fragment_path, "w", encoding="utf-8") as f:
        f.write("".join(parts))

    # Write stats to JSON file
    stats_path = os.path.join(fragment_dir, f"stats_{rank:04d}.json")
    with open(stats_path, "w", encoding="utf-8") as f:
        json.dump(prompt_stats, f)

    _print_progress(f"[Rank {rank}] Done! Generated {total_extracted}, evaluated {total_reference}.")


def main() -> None:
    args = parse_args()
    script_start = time.time()

    # Determine dp_size
    ngpus = torch.cuda.device_count()
    if args.dp_size == 0:
        dp = max(1, ngpus)
    else:
        dp = args.dp_size

    if ngpus > 0 and dp > ngpus:
        raise RuntimeError(f"Requested dp_size={dp}, but only {ngpus} CUDA devices are visible.")
    if ngpus == 0 and dp != 1:
        raise RuntimeError("No CUDA devices visible; run with --dp_size 1 for CPU mode.")

    torch.manual_seed(args.seed)
    random.seed(args.seed)
    np.random.seed(args.seed)

    # Create config (without selected_prompt_ids yet)
    cfg = Config(
        critic_path=args.critic_path,
        ref_path=args.ref_path,
        data_path=args.data_path,
        out_dir=args.out_dir,
        num_prompts=args.num_prompts,
        num_samples=args.num_samples,
        temperature=args.temperature,
        min_p=args.min_p,
        max_new_tokens=args.max_new_tokens,
        max_candidates_show=args.max_candidates_show,
        distribution_token_id=args.distribution_token_id,
        label_column=args.label_column,
        max_length=args.max_length,
        seed=args.seed,
        dtype=args.dtype,
        attn_implementation=args.attn_implementation,
        tokenizer_path=args.tokenizer_path,
        prompt_idx=args.prompt_idx,
        require_mixed_outcomes=args.require_mixed_outcomes,
        min_correct_pct=args.min_correct_pct,
        min_incorrect_pct=args.min_incorrect_pct,
        show_reference_trajectories=args.show_reference_trajectories,
        max_avg_response_length=args.max_avg_response_length,
        dp_size=dp,
        reward_col=args.label_column,
        posterior_mode=args.posterior_mode,
    )

    # Load data to determine prompts (only in main process)
    _print_progress(f"Loading data from {args.data_path}...")
    prompt_groups = _load_prompt_groups(cfg)
    prompt_ids_sorted = sorted(prompt_groups.keys())
    _print_progress(f"Loaded {len(prompt_groups)} prompts")

    # Filter by average response length
    if args.max_avg_response_length > 0:
        length_filtered_ids = [
            pid for pid in prompt_ids_sorted
            if _avg_response_length(prompt_groups[pid]["rollouts"]) <= args.max_avg_response_length
        ]
        _print_progress(f"After length filter (avg <= {args.max_avg_response_length}): {len(length_filtered_ids)}/{len(prompt_ids_sorted)} prompts")
    else:
        length_filtered_ids = prompt_ids_sorted

    # Filter for mixed outcomes if requested
    if args.require_mixed_outcomes:
        filtered_ids = [
            pid for pid in length_filtered_ids
            if _has_mixed_outcomes(
                prompt_groups[pid]["rollouts"],
                min_correct_pct=args.min_correct_pct,
                min_incorrect_pct=args.min_incorrect_pct,
            )
        ]
        _print_progress(f"After mixed outcomes filter: {len(filtered_ids)} prompts")
    else:
        filtered_ids = length_filtered_ids

    # Select prompts (randomly sample if not explicitly specified)
    if args.prompt_idx and len(args.prompt_idx) > 0:
        selected_prompt_ids = [int(pid) for pid in args.prompt_idx if int(pid) in prompt_groups]
        if args.require_mixed_outcomes:
            selected_prompt_ids = [pid for pid in selected_prompt_ids if pid in filtered_ids]
    else:
        # Randomly sample prompts instead of taking first N
        rng = random.Random(args.seed)
        if len(filtered_ids) <= args.num_prompts:
            selected_prompt_ids = filtered_ids
        else:
            selected_prompt_ids = rng.sample(filtered_ids, args.num_prompts)
        selected_prompt_ids.sort()  # Sort for consistent ordering in output

    cfg.selected_prompt_ids = selected_prompt_ids

    _print_progress(
        f"Will process {len(selected_prompt_ids)} prompts, {args.num_samples} samples each "
        f"across {dp} GPU(s)"
    )

    # Create temp directory for fragments
    fragment_dir = tempfile.mkdtemp(prefix="extract_policy_")
    _print_progress(f"Fragment directory: {fragment_dir}")

    try:
        # Spawn workers
        if dp == 1:
            # Single GPU: run directly without spawning
            _worker(0, 1, cfg, fragment_dir)
        else:
            ctx = mp.get_context("spawn")
            procs = []
            for r in range(dp):
                p = ctx.Process(target=_worker, args=(r, dp, cfg, fragment_dir), daemon=False)
                p.start()
                procs.append(p)

            # Wait for all workers
            any_fail = False
            for r, p in enumerate(procs):
                p.join()
                if p.exitcode != 0:
                    any_fail = True
                    print(f"[main] Worker rank {r} (pid={p.pid}) exited with code {p.exitcode}", flush=True)

            if any_fail:
                raise RuntimeError("One or more workers failed")

        # Create output directory
        _print_progress("Creating per-prompt HTML files...")
        os.makedirs(args.out_dir, exist_ok=True)

        if cfg.posterior_mode:
            title = "Extracted Policy Generation (Posterior: π ∝ π_ref · Q)"
        else:
            title = f"Extracted Policy Generation (τ={cfg.temperature})"

        # Build meta info HTML (shared by all files)
        meta_html = '<div class="meta">'
        meta_html += f"critic_path: {html_lib.escape(args.critic_path)}<br>"
        meta_html += f"ref_path: {html_lib.escape(args.ref_path)}<br>"
        meta_html += f"data_path: {html_lib.escape(args.data_path)}<br>"
        if cfg.posterior_mode:
            meta_html += "mode: posterior (π ∝ π_ref · Q)<br>"
        else:
            meta_html += f"mode: exp (π ∝ π_ref · exp(Q/τ)), temperature: {cfg.temperature}<br>"
        meta_html += f"min_p: {cfg.min_p}<br>"
        meta_html += f"num_prompts: {len(selected_prompt_ids)}<br>"
        meta_html += f"num_samples: {args.num_samples}<br>"
        meta_html += f"max_new_tokens: {args.max_new_tokens}<br>"
        meta_html += f"dp_size: {dp}<br>"
        meta_html += "</div>"

        # Read and collect fragments and stats
        prompt_fragments: Dict[int, str] = {}
        all_stats: Dict[int, Dict] = {}

        for r in range(dp):
            fragment_path = os.path.join(fragment_dir, f"fragment_{r:04d}.html")
            if os.path.exists(fragment_path):
                with open(fragment_path, "r", encoding="utf-8") as f:
                    content = f.read()
                # Associate with the prompts this rank processed
                rank_prompts = [pid for i, pid in enumerate(selected_prompt_ids) if i % dp == r]
                if rank_prompts and content:
                    # Split content by prompt boundaries (each prompt starts with <hr>)
                    prompt_sections = [s for s in content.split("<hr>") if s.strip()]
                    for i, section in enumerate(prompt_sections):
                        if i < len(rank_prompts):
                            prompt_fragments[rank_prompts[i]] = section

            # Read stats
            stats_path = os.path.join(fragment_dir, f"stats_{r:04d}.json")
            if os.path.exists(stats_path):
                with open(stats_path, "r", encoding="utf-8") as f:
                    stats_list = json.load(f)
                for s in stats_list:
                    all_stats[s['prompt_idx']] = s

        # Compute and print performance summary
        threshold = 0.5
        all_extracted_q = []
        all_reference_gt = []
        all_reference_q = []
        per_prompt_results = []

        for pid in selected_prompt_ids:
            if pid not in all_stats:
                continue
            s = all_stats[pid]
            ext_q = s['extracted_final_q']
            ref_gt = s['reference_gt']
            ref_q = s['reference_final_q']

            all_extracted_q.extend(ext_q)
            all_reference_gt.extend(ref_gt)
            all_reference_q.extend(ref_q)

            # Per-prompt accuracy
            ext_correct = sum(1 for q in ext_q if q >= threshold)
            ext_total = len(ext_q)
            ext_acc = ext_correct / ext_total if ext_total > 0 else 0.0

            gt_correct = sum(1 for r in ref_gt if r >= threshold)
            gt_total = len(ref_gt)
            gt_acc = gt_correct / gt_total if gt_total > 0 else 0.0

            per_prompt_results.append({
                'idx': pid,
                'ext_acc': ext_acc, 'ext_correct': ext_correct, 'ext_total': ext_total,
                'gt_acc': gt_acc, 'gt_correct': gt_correct, 'gt_total': gt_total,
            })

        # Aggregate accuracy
        agg_ext_correct = sum(1 for q in all_extracted_q if q >= threshold)
        agg_ext_total = len(all_extracted_q)
        agg_ext_acc = agg_ext_correct / agg_ext_total if agg_ext_total > 0 else 0.0

        agg_gt_correct = sum(1 for r in all_reference_gt if r >= threshold)
        agg_gt_total = len(all_reference_gt)
        agg_gt_acc = agg_gt_correct / agg_gt_total if agg_gt_total > 0 else 0.0

        agg_ref_q_correct = sum(1 for q in all_reference_q if q >= threshold)
        agg_ref_q_total = len(all_reference_q)
        agg_ref_q_acc = agg_ref_q_correct / agg_ref_q_total if agg_ref_q_total > 0 else 0.0

        # Print summary
        print('\n' + '='*60)
        print('ACCURACY STATISTICS')
        print('='*60)
        print(f'\nThreshold for "correct": final_q >= {threshold}')
        print('\n--- Per-Prompt Results ---')
        print(f'{"Prompt":<10} {"Extracted":<15} {"Ref (GT)":<15} {"Improvement":<12}')
        print('-'*52)
        for r in per_prompt_results:
            imp = r['ext_acc'] - r['gt_acc']
            print(f'{r["idx"]:<10} {r["ext_acc"]:>6.1%} ({r["ext_correct"]}/{r["ext_total"]})   '
                  f'{r["gt_acc"]:>6.1%} ({r["gt_correct"]}/{r["gt_total"]})   {imp:>+6.1%}')

        print('\n--- Aggregate Results ---')
        print(f'Extracted Policy:     {agg_ext_acc:>6.1%} ({agg_ext_correct}/{agg_ext_total})')
        print(f'Reference (Q-pred):   {agg_ref_q_acc:>6.1%} ({agg_ref_q_correct}/{agg_ref_q_total})')
        print(f'Reference (GT):       {agg_gt_acc:>6.1%} ({agg_gt_correct}/{agg_gt_total})')
        print(f'\nImprovement over reference (GT): {agg_ext_acc - agg_gt_acc:+.1%}')
        if agg_gt_acc > 0:
            print(f'Relative improvement: {agg_ext_acc / agg_gt_acc:.2f}x')
        print('='*60 + '\n')

        # Write per-prompt HTML files
        for global_i, pid in enumerate(selected_prompt_ids):
            if pid not in prompt_fragments:
                continue

            prompt_file = os.path.join(args.out_dir, f"prompt_{pid}.html")
            prompt_parts: List[str] = [_html_header(f"{title} - Prompt {pid}")]
            prompt_parts.append(meta_html)
            prompt_parts.append('<p><a href="index.html">← Back to Index</a></p>')
            prompt_parts.append("<hr>")
            prompt_parts.append(prompt_fragments[pid])
            prompt_parts.append(_html_footer())

            with open(prompt_file, "w", encoding="utf-8") as f:
                f.write("".join(prompt_parts))

            _print_progress(f"Wrote {prompt_file}")

        # Write index.html with aggregate statistics
        index_parts: List[str] = [_html_header(title)]
        index_parts.append(meta_html)

        # Add aggregate statistics
        index_parts.append('<div class="stats">')
        index_parts.append('<h3>Aggregate Statistics</h3>')
        index_parts.append('<table style="border-collapse: collapse; margin: 10px 0;">')
        index_parts.append('<tr><th style="text-align:left; padding: 4px 12px;">Policy</th>')
        index_parts.append('<th style="padding: 4px 12px;">Correct</th>')
        index_parts.append('<th style="padding: 4px 12px;">Total</th>')
        index_parts.append('<th style="padding: 4px 12px;">Accuracy</th></tr>')
        index_parts.append(f'<tr><td style="padding: 4px 12px;"><b>Extracted Policy</b></td>')
        index_parts.append(f'<td style="text-align:center;">{agg_ext_correct}</td>')
        index_parts.append(f'<td style="text-align:center;">{agg_ext_total}</td>')
        index_parts.append(f'<td style="text-align:center;"><b>{agg_ext_acc:.1%}</b></td></tr>')
        index_parts.append(f'<tr><td style="padding: 4px 12px;">Reference (Ground Truth)</td>')
        index_parts.append(f'<td style="text-align:center;">{agg_gt_correct}</td>')
        index_parts.append(f'<td style="text-align:center;">{agg_gt_total}</td>')
        index_parts.append(f'<td style="text-align:center;">{agg_gt_acc:.1%}</td></tr>')
        index_parts.append('</table>')
        if agg_gt_acc > 0:
            improvement = agg_ext_acc - agg_gt_acc
            index_parts.append(f'<p><b>Improvement: {improvement:+.1%} ({agg_ext_acc / agg_gt_acc:.2f}x)</b></p>')
        index_parts.append('</div>')

        # Per-prompt results
        index_parts.append("<h3>Per-Prompt Results</h3>")
        index_parts.append("<ul>")
        for r in per_prompt_results:
            pid = r['idx']
            if pid in prompt_fragments:
                imp = r['ext_acc'] - r['gt_acc']
                index_parts.append(
                    f'<li><a href="prompt_{pid}.html">Prompt {pid}</a>: '
                    f'Extracted {r["ext_acc"]:.0%} vs Reference {r["gt_acc"]:.0%} '
                    f'({imp:+.0%})</li>'
                )
        index_parts.append("</ul>")
        index_parts.append(_html_footer())

        index_file = os.path.join(args.out_dir, "index.html")
        with open(index_file, "w", encoding="utf-8") as f:
            f.write("".join(index_parts))

        total_time = time.time() - script_start
        _print_progress(
            f"Done! Wrote {len(prompt_fragments)} prompt files to {args.out_dir}/\n"
            f"  Index: {index_file}\n"
            f"  Total time: {total_time:.1f}s"
        )

    finally:
        # Cleanup fragment directory
        try:
            shutil.rmtree(fragment_dir)
        except Exception:
            pass


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("Interrupted.", flush=True)
    except Exception:
        traceback.print_exc()
        sys.exit(1)
